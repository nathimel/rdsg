{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotnine as pn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "params = [\n",
    "    0, 1, 2, 4, 8, 16, # nosofsky\n",
    "    # 0.1, 0.2, 0.5, 0.6, 0.75, 1, 2, 3, 5, 1000 # exp\n",
    "]\n",
    "\n",
    "# Learned\n",
    "# dynamic = \"reinforcement_learning\"\n",
    "dynamic = \"replicator_dynamic\"\n",
    "\n",
    "# num_trials = 10\n",
    "num_trials = 100\n",
    "\n",
    "# similarity = \"nosofsky\"\n",
    "similarity = \"nosofsky_normed\"\n",
    "# similarity = \"exp\"\n",
    "# similarity = \"exp_normed\"\n",
    "\n",
    "distortion = \"squared_dist\"\n",
    "# distortion = \"abs_dist\"\n",
    "\n",
    "out = \"multirun\"\n",
    "# out = \"outputs\"\n",
    "\n",
    "# trajectory = True\n",
    "trajectory = False\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# Below three saved in outputs, not multirun\n",
    "\n",
    "# Pareto frontier\n",
    "curve_fn = f\"/Users/nathanielimel/lps/projects/rdsg/outputs/states=10/signals=10/distortion={distortion}/curve_points.csv\"\n",
    "\n",
    "# Counterpart points\n",
    "counterpart_fn = f\"/Users/nathanielimel/lps/projects/rdsg/outputs/states=10/signals=10/distortion={distortion}/counterpart_points.csv\"\n",
    "\n",
    "# Explored hypothetical systems\n",
    "sampled_fn = f\"/Users/nathanielimel/lps/projects/rdsg/outputs/states=10/signals=10/distortion={distortion}/sampled_points.csv\"\n",
    "\n",
    "#############################################################################\n",
    "# Typically saved in multirun\n",
    "# Simulation points\n",
    "param_fns = {\n",
    "    f\"{float(param)}\":f\"/Users/nathanielimel/lps/projects/rdsg/{out}/states=10/signals=10/distortion={distortion}/similarity={similarity}/dynamics={dynamic}/num_trials={num_trials}/sim_param={param}/simulation_points.csv\"\n",
    "    for param in params\n",
    "}\n",
    "\n",
    "variant_fns = {key: val.replace(\"simulation_points\", \"variants_points\") for key, val in param_fns.items()}\n",
    "\n",
    "if trajectory:\n",
    "    param_fns = {key: val.replace(\"simulation_points\", \"mean_points\") for key, val in param_fns.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframes\n",
    "curve_data = pd.read_csv(curve_fn)\n",
    "counterpart_data = pd.read_csv(counterpart_fn)\n",
    "sampled_data = pd.read_csv(sampled_fn)\n",
    "\n",
    "simulation_dataframes = {\n",
    "    k: pd.read_csv(param_fns[k]) for k in param_fns\n",
    "}\n",
    "\n",
    "variant_dataframes = {\n",
    "    k: pd.read_csv(variant_fns[k]) for k in variant_fns\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = {\"exp\": \"beta\", \"nosofsky\": \"alpha\"}\n",
    "for name in param_names:\n",
    "    if name in similarity:\n",
    "        param_name = param_names[name]\n",
    "\n",
    "# combine simulation dataframes\n",
    "for key in simulation_dataframes:\n",
    "    df = simulation_dataframes[key]\n",
    "    df[param_name] = key # use string for category\n",
    "\n",
    "sim_data = pd.concat(simulation_dataframes.values())\n",
    "sim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine variant dataframes\n",
    "# combine simulation dataframes\n",
    "for key in variant_dataframes:\n",
    "    df = variant_dataframes[key]\n",
    "    df[param_name] = key # use string for category\n",
    "\n",
    "variant_data = pd.concat(variant_dataframes.values())\n",
    "variant_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Cast to int if all params are ints\n",
    "\n",
    "sim_data[param_name] = sim_data[param_name].astype(float)\n",
    "sim_data[param_name] = sim_data[param_name].astype(int)\n",
    "sim_data[param_name] = sim_data[param_name].astype(str)\n",
    "\n",
    "variant_data[param_name] = variant_data[param_name].astype(float)\n",
    "variant_data[param_name] = variant_data[param_name].astype(int)\n",
    "variant_data[param_name] = variant_data[param_name].astype(str)\n",
    "\n",
    "counterpart_data[param_name] = counterpart_data[param_name].astype(float)\n",
    "counterpart_data[param_name] = counterpart_data[param_name].astype(int)\n",
    "counterpart_data[param_name] = counterpart_data[param_name].astype(str)\n",
    "\n",
    "sim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot each language, with a unique color for each gamma\n",
    "\n",
    "# hard to clean this up with pd.Categorical\n",
    "if param_name == \"alpha\":\n",
    "    counterpart_data = counterpart_data.assign(\n",
    "            alpha=pd.Categorical(\n",
    "                counterpart_data[\"alpha\"], \n",
    "                categories=[str(param) for param in params]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    sim_data = sim_data.assign(\n",
    "            alpha=pd.Categorical(\n",
    "                sim_data[\"alpha\"], \n",
    "                categories=[str(param) for param in params]\n",
    "            )\n",
    "        )\n",
    "else:\n",
    "    counterpart_data = counterpart_data.assign(\n",
    "            beta=pd.Categorical(\n",
    "                counterpart_data[\"beta\"], \n",
    "                categories=[str(param) for param in params]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    sim_data = sim_data.assign(\n",
    "            beta=pd.Categorical(\n",
    "                sim_data[\"beta\"], \n",
    "                categories=[str(param) for param in params]\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "# Language keys\n",
    "sim_data[\"language\"] = {\"reinforcement_learning\": \"learned\", \"replicator_dynamic\": \"evolved\"}[dynamic]\n",
    "# counterpart_data[\"language\"] = \"optimal counterpart\"\n",
    "counterpart_data[\"language\"] = \"optimal \\ncounterpart\"\n",
    "all_data = pd.concat([counterpart_data, sim_data])\n",
    "\n",
    "sampled_data[\"language\"] = \"hypothetical\"\n",
    "variant_data[\"language\"] = \"hypothetical\"\n",
    "\n",
    "# Trajectory keys\n",
    "if trajectory:\n",
    "    sim_data[\"time step\"] = sim_data[\"round\"]\n",
    "\n",
    "counterpart_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = (\n",
    "    # Set data and the axes\n",
    "    pn.ggplot(\n",
    "        data=curve_data, mapping=pn.aes(x=\"rate\", y=\"distortion\")\n",
    "    )  \n",
    "    + pn.geom_point(  # sampled langs\n",
    "        # sampled_data,\n",
    "        variant_data,\n",
    "        pn.aes(shape=\"language\"),\n",
    "        color=\"gray\",\n",
    "        size=5,\n",
    "        alpha=0.2,\n",
    "    )\n",
    "    + pn.geom_line(size=1) # pareto data                   \n",
    "    + pn.geom_jitter( # simulation langs\n",
    "        data=sim_data,\n",
    "        mapping=pn.aes(\n",
    "            color=param_name, \n",
    "            shape=\"language\",\n",
    "            ),\n",
    "        alpha=0.3,\n",
    "        size=5,\n",
    "        # height=0.1,\n",
    "    )\n",
    "    + pn.geom_point( # theoretical bound langs last\n",
    "        data=counterpart_data,\n",
    "        mapping=pn.aes(\n",
    "            color=param_name,\n",
    "            shape=\"language\",\n",
    "            ),\n",
    "        size=5,\n",
    "    )    \n",
    "    + pn.xlab(\"Complexity $I(S;\\hat{S})$\")\n",
    "    + pn.ylab(\"Communicative Cost $D[S, \\hat{S}]$\")\n",
    "    + pn.theme_seaborn(context=\"paper\")\n",
    "    + pn.theme(text=pn.element_text(size=18))\n",
    ")\n",
    "print(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save plot (will overwrite!)\n",
    "\n",
    "plot_save_dir = f\"/Users/nathanielimel/lps/projects/rdsg/multirun/states=10/signals=10/distortion={distortion}/similarity={similarity}/dynamics={dynamic}/num_trials={num_trials}\"\n",
    "fn = plot_save_dir + f\"/multiple_{param_name}.png\"\n",
    "plot.save(filename=fn, width=10, height=10, dpi=300)\n",
    "print(fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "mean_traj_fns = {\n",
    "    key: val.replace(\"simulation_points\", \"mean_points\") for key, val in param_fns.items()\n",
    "}\n",
    "\n",
    "mean_traj_dataframes = {\n",
    "    k: pd.read_csv(mean_traj_fns[k]) for k in mean_traj_fns\n",
    "}\n",
    "\n",
    "# combine simulation dataframes\n",
    "for key in mean_traj_dataframes:\n",
    "    df = mean_traj_dataframes[key]\n",
    "    df[\"alpha\"] = key # use string for category\n",
    "\n",
    "mean_traj_data = pd.concat(mean_traj_dataframes.values())\n",
    "mean_traj_data = mean_traj_data.dropna() # unnecessary if all files were trajectories\n",
    "\n",
    "mean_traj_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_traj_data[\"alpha\"] = mean_traj_data[\"alpha\"].astype(float)\n",
    "mean_traj_data[\"alpha\"] = mean_traj_data[\"alpha\"].astype(int)\n",
    "mean_traj_data[\"alpha\"] = mean_traj_data[\"alpha\"].astype(str)\n",
    "\n",
    "mean_traj_data = mean_traj_data.assign(\n",
    "    alpha=pd.Categorical(\n",
    "        mean_traj_data[\"alpha\"],\n",
    "        categories=[str(param) for param in params]\n",
    "    )\n",
    ")\n",
    "\n",
    "mean_traj_data[\"language\"] = {\"reinforcement_learning\": \"learned\", \"replicator_dynamic\": \"evolved\"}[dynamic]\n",
    "\n",
    "mean_traj_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the final rounds\n",
    "\n",
    "final_round = mean_traj_data[\"round\"].max()\n",
    "final_round_data = mean_traj_data[mean_traj_data[\"round\"] == final_round]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try latex for color\n",
    "\n",
    "# r'$\\sin (x)$'\n",
    "\n",
    "final_round_data[\"$\\alpha$\"] = final_round_data[\"alpha\"]\n",
    "mean_traj_data[\"$\\alpha$\"] = mean_traj_data[\"alpha\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = (\n",
    "    # Set data and the axes\n",
    "    pn.ggplot(\n",
    "            data=curve_data, mapping=pn.aes(x=\"rate\", y=\"distortion\")\n",
    "    )  # pareto data    \n",
    "    + pn.geom_point(  # sampled langs\n",
    "        # sampled_data,\n",
    "        variant_data,\n",
    "        pn.aes(shape=\"language\"),\n",
    "        color=\"gray\",\n",
    "        size=5,\n",
    "        alpha=0.2,\n",
    "    )\n",
    "    + pn.geom_line(  # simulation langs\n",
    "        data=mean_traj_data,\n",
    "        mapping=pn.aes(color=\"alpha\"),\n",
    "        # shape=\"o\",\n",
    "        alpha=1.0,\n",
    "        size=3,\n",
    "    )\n",
    "    + pn.geom_line(size=1) # pareto     \n",
    "    + pn.geom_point(  # final langs\n",
    "        data=final_round_data,\n",
    "        mapping=pn.aes(color=\"alpha\"),\n",
    "        shape=\"X\",\n",
    "        alpha=1.0,\n",
    "        size=5,\n",
    "    )\n",
    "    + pn.xlab(\"Complexity $I(S;\\hat{S})$\")\n",
    "    + pn.ylab(\"Communicative Cost $D[S, \\hat{S}]$\")\n",
    "    + pn.theme_seaborn(context=\"paper\")\n",
    "    + pn.theme(text=pn.element_text(size=18))\n",
    ")\n",
    "print(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save plot (will overwrite!)\n",
    "\n",
    "plot_save_dir = f\"/Users/nathanielimel/lps/projects/rdsg/multirun/states=10/signals=10/distortion={distortion}/similarity={similarity}/dynamics={dynamic}/num_trials={num_trials}\"\n",
    "fn = plot_save_dir + \"/alpha_trajectories.png\"\n",
    "plot.save(filename=fn, width=10, height=10, dpi=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform statistical analyses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure optimality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N.B.: The reason we need to copypaste this function is because it is sometimes more convenient to measure optimality all at once here, instead of running python script in a hydra sweep (if I forgot to)\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def measure_optimality(data: pd.DataFrame, curve_data: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Compute the min distance to any point on the frontier, for every point. Requires `data` to contain more than one row.\"\"\"\n",
    "    # get curve points as list of pairs\n",
    "    pareto_points = np.array(list(curve_data[[\"rate\", \"distortion\"]].itertuples(index=False, name=None)))\n",
    "    points = np.array(list(data[[\"rate\", \"distortion\"]].itertuples(index=False, name=None)))\n",
    "    # N.B.: do not interpolate, so you don't measure high-dist random langs as more optimal than they are!\n",
    "\n",
    "    # Measure closeness of each language to any frontier point\n",
    "    distances = cdist(points, pareto_points)\n",
    "    min_distances = np.min(distances, axis=1)\n",
    "\n",
    "    # max complexity will be achieved by B-A    \n",
    "    max_complexity = pareto_points[:, 0].max()\n",
    "    # points may have higher cost than pareto max cost    \n",
    "    max_cost = max(points[:,1].max(), pareto_points[:,].max())\n",
    "    # max possible distance is sqrt( max_rate^2 + (max_distortion)^2 )\n",
    "    max_distance = np.sqrt(max_cost**2 + max_complexity**2)\n",
    "\n",
    "    # just use the max found distance? \n",
    "    # max_distance = np.max(min_distances)\n",
    "\n",
    "    min_distances /= max_distance\n",
    "    optimalities = 1 - min_distances\n",
    "    return optimalities\n",
    "    \n",
    "    # return min_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data[\"optimality\"] = measure_optimality(sim_data, curve_data)\n",
    "variant_data[\"optimality\"] = measure_optimality(variant_data, curve_data)\n",
    "# sampled_data[\"optimality\"] = measure_optimality(sampled_data, curve_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sim_data.mean())\n",
    "display(variant_data.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sim_data.min())\n",
    "display(variant_data.min())\n",
    "display(sampled_data.min())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show optimality in violin plots\n",
    "*N.B.: need to change python interpreter to 'seaborn'!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N.B.: good violin plots requires seaborn 0.12.2, which is incompatible with plotnine\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.concat([sim_data, variant_data])\n",
    "\n",
    "violin = sns.catplot(\n",
    "    data=data, y=\"alpha\", x=\"optimality\", hue=\"language\", kind=\"violin\", inner=None, split=False, cut=0, bw=.15, palette=\"pastel\"\n",
    ")\n",
    "\n",
    "# violin.set(xlim=(None, None))\n",
    "\n",
    "# sns.catplot(\n",
    "#     data=tips, x=\"day\", y=\"total_bill\", hue=\"sex\",\n",
    "#     kind=\"violin\", inner=\"stick\", split=True, palette=\"pastel\",\n",
    "# )\n",
    "\n",
    "plt.show()\n",
    "# plt.savefig(\"rep_violin.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### be messy and load up rl data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up rl data by hand, not from saved\n",
    "\n",
    "# Simulation points\n",
    "param_fns_rl = {\n",
    "    f\"{float(param)}\":f\"/Users/nathanielimel/lps/projects/rdsg/{out}/states=10/signals=10/distortion={distortion}/similarity={similarity}/dynamics=reinforcement_learning/num_trials={num_trials}/sim_param={param}/simulation_points.csv\"\n",
    "    for param in params\n",
    "}\n",
    "\n",
    "variant_fns_rl = {key: val.replace(\"simulation_points\", \"variants_points\") for key, val in param_fns.items()}\n",
    "\n",
    "\n",
    "simulation_dataframes_rl = {\n",
    "    k: pd.read_csv(param_fns_rl[k]) for k in param_fns_rl\n",
    "}\n",
    "\n",
    "variant_dataframes_rl = {\n",
    "    k: pd.read_csv(variant_fns_rl[k]) for k in variant_fns_rl\n",
    "}\n",
    "\n",
    "\n",
    "# combine simulation dataframes\n",
    "for key in simulation_dataframes_rl:\n",
    "    df = simulation_dataframes_rl[key]\n",
    "    df[param_name] = key # use string for category\n",
    "\n",
    "sim_data_rl = pd.concat(simulation_dataframes_rl.values())\n",
    "display(sim_data_rl)\n",
    "\n",
    "\n",
    "# combine\n",
    "for key in variant_dataframes_rl:\n",
    "    df = variant_dataframes_rl[key]\n",
    "    df[param_name] = key # use string for category\n",
    "\n",
    "variant_data_rl = pd.concat(variant_dataframes_rl.values())\n",
    "display(variant_data_rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more messiness\n",
    "\n",
    "sim_data_rl[\"optimality\"] = measure_optimality(sim_data_rl, curve_data)\n",
    "variant_data_rl[\"optimality\"] = measure_optimality(variant_data_rl, curve_data)\n",
    "\n",
    "\n",
    "sim_data_rl[param_name] = sim_data_rl[param_name].astype(float)\n",
    "sim_data_rl[param_name] = sim_data_rl[param_name].astype(int)\n",
    "sim_data_rl[param_name] = sim_data_rl[param_name].astype(str)\n",
    "\n",
    "variant_data_rl[param_name] = variant_data_rl[param_name].astype(float)\n",
    "variant_data_rl[param_name] = variant_data_rl[param_name].astype(int)\n",
    "variant_data_rl[param_name] = variant_data_rl[param_name].astype(str)\n",
    "\n",
    "\n",
    "sim_data_rl = sim_data_rl.assign(\n",
    "            alpha=pd.Categorical(\n",
    "                sim_data_rl[\"alpha\"], \n",
    "                categories=[str(param) for param in params]\n",
    "            )\n",
    "        )\n",
    "\n",
    "variant_data_rl = variant_data_rl.assign(\n",
    "            alpha=pd.Categorical(\n",
    "                variant_data_rl[\"alpha\"],\n",
    "                categories=[str(param) for param in params]\n",
    "            )\n",
    ")\n",
    "\n",
    "\n",
    "sim_data_rl[\"language\"] = \"learned\"\n",
    "variant_data_rl[\"language\"] = \"hypothetical\"\n",
    "\n",
    "sim_data_rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get violin for both\n",
    "rep_data = pd.concat([sim_data, variant_data])\n",
    "rl_data = pd.concat([sim_data_rl, variant_data_rl])\n",
    "dynamics_data = pd.concat([rep_data, rl_data])\n",
    "\n",
    "# change language to categorical to customize ordering\n",
    "dynamics_data = dynamics_data.assign(\n",
    "        language=pd.Categorical(\n",
    "                dynamics_data[\"language\"], \n",
    "                categories=[\"learned\", \"evolved\", \"hypothetical\"]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "violin = sns.catplot(\n",
    "    data=dynamics_data,\n",
    "    x=\"alpha\", y=\"optimality\", \n",
    "    hue=\"language\", \n",
    "    kind=\"violin\", \n",
    "    inner=None, \n",
    "    split=False, \n",
    "    cut=0, \n",
    "    bw=.05, \n",
    "    palette=\"pastel\",\n",
    "    scale=\"width\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamics_data = dynamics_data.assign(\n",
    "        alpha=pd.Categorical(\n",
    "                dynamics_data[\"alpha\"], \n",
    "                categories=[str(param) for param in params]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try plotnine for violins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want alpha to be floats again\n",
    "\n",
    "values = {\"learned\": \"orange\", \"evolved\": \"green\", \"hypothetical\": \"blue\"}\n",
    "\n",
    "plot = (\n",
    "    # Set data and the axes\n",
    "    pn.ggplot(\n",
    "            data=dynamics_data, mapping=pn.aes(x=\"alpha\", y=\"optimality\")\n",
    "    )\n",
    "    + pn.geom_violin(\n",
    "        data=dynamics_data,\n",
    "        mapping=pn.aes(\n",
    "            fill=\"language\", \n",
    "            color=\"language\", \n",
    "            ),\n",
    "        scale=\"width\",\n",
    "        bw=0.0015,\n",
    "    )\n",
    "    + pn.scale_color_manual(values=values)\n",
    "    + pn.scale_fill_manual(values=values)\n",
    "    + pn.xlab(\"Perceptual imprecision ($\\\\alpha$)\")\n",
    "    + pn.ylab(\"Optimality (Pareto-closeness)\")\n",
    "    + pn.theme_seaborn(context='paper')\n",
    "    + pn.theme(text=pn.element_text(size=18))\n",
    ")\n",
    "print(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save plot\n",
    "fn = \"optimality.png\"\n",
    "plot.save(filename=fn, width=10, height=10, dpi=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does hypothetical look like?\n",
    "data = pd.concat([sim_data, variant_data])\n",
    "\n",
    "sns.displot(data=data, x=\"rate\", y=\"deviation from optimality\", hue=\"language\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data for messing around later with plots\n",
    "\n",
    "sim_data.to_csv(\"rl.csv\", index=False)\n",
    "# sampled_data.to_csv(\"explored.csv\")\n",
    "variant_data.to_csv(\"rl_variants.csv\", index=False)\n",
    "\n",
    "\n",
    "# sim_data.to_csv(\"rep_dyn.csv\", index=False)\n",
    "# variant_data.to_csv(\"rep_dyn_variants.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure with subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patchworklib as pw\n",
    "print(pw.__version__)\n",
    "print(pn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot1 = (\n",
    "    # Set data and the axes\n",
    "    pn.ggplot(\n",
    "        data=curve_data, mapping=pn.aes(x=\"rate\", y=\"distortion\")\n",
    "    )  \n",
    "    + pn.geom_point(  # sampled langs\n",
    "        # sampled_data,\n",
    "        variant_data,\n",
    "        pn.aes(shape=\"language\"),\n",
    "        color=\"gray\",\n",
    "        size=5,\n",
    "        alpha=0.2,\n",
    "    )\n",
    "    + pn.geom_line(size=1) # pareto data                   \n",
    "    + pn.geom_jitter( # simulation langs\n",
    "        data=sim_data,\n",
    "        mapping=pn.aes(\n",
    "            color=param_name, \n",
    "            shape=\"language\",\n",
    "            ),\n",
    "        alpha=0.3,\n",
    "        size=5,\n",
    "        # height=0.1,\n",
    "    )\n",
    "    + pn.geom_point( # theoretical bound langs last\n",
    "        data=counterpart_data,\n",
    "        mapping=pn.aes(\n",
    "            color=param_name,\n",
    "            shape=\"language\",\n",
    "            ),\n",
    "        size=5,\n",
    "    )    \n",
    "    + pn.xlab(\"Complexity $I(S;\\hat{S})$\")\n",
    "    + pn.ylab(\"Communicative Cost $D[S, \\hat{S}]$\")\n",
    "    + pn.ggtitle(\"Roth-Erev Reinforcement Learning\")\n",
    "    # + pn.theme_seaborn(context=\"paper\")\n",
    "    # + pn.theme(text=pn.element_text(size=18))\n",
    "    + pn.theme(\n",
    "                legend_title=pn.element_blank(),\n",
    "                figure_size=(12, 9),\n",
    "                legend_position='none',\n",
    "                legend_box='horizontal',\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot2 = (\n",
    "    # Set data and the axes\n",
    "    pn.ggplot(\n",
    "        data=curve_data, mapping=pn.aes(x=\"rate\", y=\"distortion\")\n",
    "    )  \n",
    "    + pn.geom_point(  # sampled langs\n",
    "        # sampled_data,\n",
    "        variant_data_rl,\n",
    "        pn.aes(shape=\"language\"),\n",
    "        color=\"gray\",\n",
    "        size=5,\n",
    "        alpha=0.2,\n",
    "    )\n",
    "    + pn.geom_line(size=1) # pareto data                   \n",
    "    + pn.geom_jitter( # simulation langs\n",
    "        data=sim_data,\n",
    "        mapping=pn.aes(\n",
    "            color=param_name, \n",
    "            shape=\"language\",\n",
    "            ),\n",
    "        alpha=0.3,\n",
    "        size=5,\n",
    "        # height=0.1,\n",
    "    )\n",
    "    + pn.geom_point( # theoretical bound langs last\n",
    "        data=counterpart_data,\n",
    "        mapping=pn.aes(\n",
    "            color=param_name,\n",
    "            shape=\"language\",\n",
    "            ),\n",
    "        size=5,\n",
    "    )    \n",
    "    + pn.xlab(\"Complexity $I(S;\\hat{S})$\")\n",
    "    + pn.ylab(\"Communicative Cost $D[S, \\hat{S}]$\")\n",
    "    + pn.ggtitle(\"Replicator Dynamic\")\n",
    "    # + pn.theme_seaborn(context=\"paper\")\n",
    "    # + pn.theme(text=pn.element_text(size=18))\n",
    "    + pn.theme(\n",
    "                legend_title=pn.element_blank(),\n",
    "                figure_size=(12, 9),\n",
    "                legend_position='right',\n",
    "                legend_box='horizontal',\n",
    "                legend_text=pn.element_text(size=18)\n",
    "        )\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine figs\n",
    "\n",
    "plot1 = pw.load_ggplot(plot1, figsize=(4,3))\n",
    "plot2 = pw.load_ggplot(plot2, figsize=(4,3))\n",
    "\n",
    "g = plot1 | plot2\n",
    "g.savefig(\"example.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdsg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b463abd35cf580a02b634b7a73a252c5d4b2192487c346d673c6979477c61569"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
